{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f92e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "ENERGY CONSUMPTION PREPROCESSING - CORRECTED PIPELINE (v2)\n",
      "==========================================================================================\n",
      "\n",
      "1. LOADING FULL DATASET\n",
      "------------------------------------------------------------------------------------------\n",
      "‚úì Training data: 20,216,100 rows (~1.49 GB)\n",
      "‚úì Buildings: 1449\n",
      "‚úì Building metadata: (1449, 6)\n",
      "‚úì Weather data: (139773, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"ENERGY CONSUMPTION PREPROCESSING - CORRECTED PIPELINE (v2)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD FULL DATASET (NO BUILDING REMOVAL)\n",
    "# ============================================================================\n",
    "print(\"\\n1. LOADING FULL DATASET\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    r\"C:\\Users\\Madhw\\Downloads\\Energy-consumption-forecasting-saatwik\\Energy-consumption-forecasting-saatwik\\dataset\\train.csv\",\n",
    "    dtype={'building_id': 'int16', 'meter': 'int8'}\n",
    ")\n",
    "\n",
    "building_df = pd.read_csv(\n",
    "    r\"C:\\Users\\Madhw\\Downloads\\Energy-consumption-forecasting-saatwik\\Energy-consumption-forecasting-saatwik\\dataset\\building_metadata.csv\",\n",
    "    dtype={'building_id': 'int16', 'site_id': 'int8'}\n",
    ")\n",
    "\n",
    "weather_df = pd.read_csv(\n",
    "    r\"C:\\Users\\Madhw\\Downloads\\Energy-consumption-forecasting-saatwik\\Energy-consumption-forecasting-saatwik\\dataset\\weather_train.csv\",\n",
    "    dtype={'site_id': 'int8'}\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training data: {train_df.shape[0]:,} rows (~{train_df.memory_usage(deep=True).sum()/1024**3:.2f} GB)\")\n",
    "print(f\"‚úì Buildings: {train_df['building_id'].nunique()}\")\n",
    "print(f\"‚úì Building metadata: {building_df.shape}\")\n",
    "print(f\"‚úì Weather data: {weather_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1783630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True C:\\Users\\Madhw\\Downloads\\Energy-consumption-forecasting-saatwik\\Energy-consumption-forecasting-saatwik\\dataset\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\Madhw\\Downloads\\Energy-consumption-forecasting-saatwik\\Energy-consumption-forecasting-saatwik\\dataset\\train.csv\"\n",
    "print(os.path.exists(path), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fdd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. DEDUPLICATION & FREQUENCY REGULARIZATION\n",
      "------------------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Duplicate (building_id, timestamp) pairs BEFORE: 7,822,101\n",
      "‚úì Duplicate pairs AFTER: 0\n",
      "‚úì Rows after dedup: 12,393,999\n",
      "\n",
      "‚úì Checking frequency regularity...\n",
      "   Mean records per building: 8553\n",
      "   Min: 479, Max: 8784\n",
      "‚ö†Ô∏è  Found 3814 gaps of 1-2 hours (leaving as-is for interpolation)\n",
      "‚úì Training period: 2016-01-01 00:00:00 to 2016-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: DEDUPLICATE & REGULARIZE FREQUENCY\n",
    "# ============================================================================\n",
    "print(\"\\n2. DEDUPLICATION & FREQUENCY REGULARIZATION\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Convert timestamps\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])\n",
    "\n",
    "# Check for duplicates BEFORE deduplication\n",
    "dup_count_before = train_df.duplicated(subset=['building_id', 'timestamp']).sum()\n",
    "print(f\"‚ö†Ô∏è  Duplicate (building_id, timestamp) pairs BEFORE: {dup_count_before:,}\")\n",
    "\n",
    "# Deduplicate: sum meter_reading for same building+timestamp (represents multiple meters)\n",
    "# Group by building_id + timestamp, sum meter_reading, and take first of other columns\n",
    "train_df = train_df.groupby(['building_id', 'timestamp']).agg({\n",
    "    'meter_reading': 'sum',  # Sum all readings at same timestamp (multi-meter aggregation)\n",
    "    'meter': 'first'         # Meter type (should be same for same building)\n",
    "}).reset_index()\n",
    "\n",
    "dup_count_after = train_df.duplicated(subset=['building_id', 'timestamp']).sum()\n",
    "print(f\"‚úì Duplicate pairs AFTER: {dup_count_after:,}\")\n",
    "print(f\"‚úì Rows after dedup: {len(train_df):,}\")\n",
    "\n",
    "# Sort chronologically (CRITICAL for time series)\n",
    "train_df = train_df.sort_values(['building_id', 'timestamp']).reset_index(drop=True)\n",
    "weather_df = weather_df.sort_values(['site_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Check frequency regularization per building\n",
    "print(f\"\\n‚úì Checking frequency regularity...\")\n",
    "freq_counts = train_df.groupby('building_id')['timestamp'].apply(lambda x: len(x))\n",
    "print(f\"   Mean records per building: {freq_counts.mean():.0f}\")\n",
    "print(f\"   Min: {freq_counts.min()}, Max: {freq_counts.max()}\")\n",
    "\n",
    "# Forward-fill short gaps (1-2 hours) within each building\n",
    "train_df = train_df.sort_values(['building_id', 'timestamp'])\n",
    "train_df['time_gap'] = train_df.groupby('building_id')['timestamp'].diff().dt.total_seconds() / 3600\n",
    "gap_hours_1_2 = (train_df['time_gap'] > 1) & (train_df['time_gap'] <= 2)\n",
    "if gap_hours_1_2.sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {gap_hours_1_2.sum()} gaps of 1-2 hours (leaving as-is for interpolation)\")\n",
    "train_df = train_df.drop('time_gap', axis=1)\n",
    "\n",
    "print(f\"‚úì Training period: {train_df['timestamp'].min()} to {train_df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4d00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. MERGING METADATA & WEATHER\n",
      "------------------------------------------------------------------------------------------\n",
      "‚úì Merged building metadata: (12393999, 9)\n",
      "‚úì Merged weather data: (12393999, 16)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: MERGE METADATA & WEATHER (NO BUILDING REMOVAL)\n",
    "# ============================================================================\n",
    "print(\"\\n3. MERGING METADATA & WEATHER\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Merge building metadata\n",
    "train_df = train_df.merge(building_df, on='building_id', how='left')\n",
    "print(f\"‚úì Merged building metadata: {train_df.shape}\")\n",
    "\n",
    "# Merge weather data\n",
    "train_df = train_df.merge(weather_df, on=['site_id', 'timestamp'], how='left')\n",
    "print(f\"‚úì Merged weather data: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a99c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original meter_reading intact\n",
    "train_df['meter_reading_log'] = np.log1p(train_df['meter_reading'])\n",
    "TARGET = 'meter_reading_log'  # Use this column for training only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3634750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. FEATURE IMPUTATION - SITE-WISE\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Missing values before imputation:\n",
      "  year_built: 6,688,083 (54.0%)\n",
      "  floor_count: 9,389,335 (75.8%)\n",
      "  air_temperature: 51,670 (0.4%)\n",
      "  cloud_coverage: 5,497,456 (44.4%)\n",
      "  dew_temperature: 53,548 (0.4%)\n",
      "  precip_depth_1_hr: 2,613,910 (21.1%)\n",
      "  sea_level_pressure: 1,032,021 (8.3%)\n",
      "  wind_direction: 699,842 (5.6%)\n",
      "  wind_speed: 71,944 (0.6%)\n",
      "\n",
      "‚úì Imputation complete - all weather features filled\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: INTELLIGENT FEATURE IMPUTATION (NOT AGGRESSIVE REMOVAL)\n",
    "# ============================================================================\n",
    "print(\"\\n4. FEATURE IMPUTATION - SITE-WISE\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "missing_before = train_df.isnull().sum()\n",
    "print(f\"\\nMissing values before imputation:\")\n",
    "for col in missing_before[missing_before > 0].index:\n",
    "    print(f\"  {col}: {missing_before[col]:,} ({missing_before[col]/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Site-wise median imputation for weather variables\n",
    "weather_cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "\n",
    "for col in weather_cols:\n",
    "    if col in train_df.columns:\n",
    "        # Fill with site-wise median\n",
    "        train_df[col] = train_df.groupby('site_id')[col].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "        # Then fill any remaining with global median\n",
    "        train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "# Building features - forward fill or global median\n",
    "building_cols = ['year_built', 'square_feet', 'building_age']\n",
    "for col in building_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\n‚úì Imputation complete - all weather features filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903fb5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. ADVANCED OUTLIER REMOVAL\n",
      "------------------------------------------------------------------------------------------\n",
      "Before outlier removal:\n",
      "  Total rows: 12,393,999\n",
      "  Meter reading - Mean: 3453.28\n",
      "  Meter reading - Std: 195705.68\n",
      "  Meter reading - Max: 21905470.91\n",
      "\n",
      "  IQR method: 102,600 extreme outliers detected (0.83%)\n",
      "  Percentile capping: 122,911 high values capped, 0 low values capped\n",
      "\n",
      "After outlier removal:\n",
      "  Total rows: 12,291,399 (removed 102,600)\n",
      "  Meter reading - Mean: 577.59\n",
      "  Meter reading - Std: 1387.48 (‚Üì99.3%)\n",
      "  Meter reading - Max: 9555.98\n",
      "  Data size: ~1.94 GB\n",
      "  Buildings: 1449\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: ADVANCED OUTLIER REMOVAL - REDUCE VARIANCE\n",
    "# ============================================================================\n",
    "print(\"\\n5. ADVANCED OUTLIER REMOVAL\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train_df_agg = train_df.copy()\n",
    "\n",
    "# Calculate statistics before outlier removal\n",
    "print(f\"Before outlier removal:\")\n",
    "print(f\"  Total rows: {len(train_df_agg):,}\")\n",
    "print(f\"  Meter reading - Mean: {train_df_agg['meter_reading'].mean():.2f}\")\n",
    "print(f\"  Meter reading - Std: {train_df_agg['meter_reading'].std():.2f}\")\n",
    "print(f\"  Meter reading - Max: {train_df_agg['meter_reading'].max():.2f}\")\n",
    "\n",
    "# Method 1: Remove extreme outliers using IQR method (per building to preserve temporal patterns)\n",
    "Q1 = train_df_agg.groupby('building_id')['meter_reading'].transform(lambda x: x.quantile(0.25))\n",
    "Q3 = train_df_agg.groupby('building_id')['meter_reading'].transform(lambda x: x.quantile(0.75))\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR  # 3x IQR for extreme outliers only\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "outlier_mask = (train_df_agg['meter_reading'] < lower_bound) | (train_df_agg['meter_reading'] > upper_bound)\n",
    "outliers_removed = outlier_mask.sum()\n",
    "print(f\"\\n  IQR method: {outliers_removed:,} extreme outliers detected ({outliers_removed/len(train_df_agg)*100:.2f}%)\")\n",
    "\n",
    "train_df_agg = train_df_agg[~outlier_mask].reset_index(drop=True)\n",
    "\n",
    "# Method 2: Cap remaining extreme values using percentiles (preserve temporal continuity)\n",
    "percentile_99 = train_df_agg['meter_reading'].quantile(0.99)\n",
    "percentile_01 = train_df_agg['meter_reading'].quantile(0.01)\n",
    "\n",
    "extreme_high = (train_df_agg['meter_reading'] > percentile_99).sum()\n",
    "extreme_low = (train_df_agg['meter_reading'] < percentile_01).sum()\n",
    "\n",
    "train_df_agg['meter_reading'] = train_df_agg['meter_reading'].clip(lower=percentile_01, upper=percentile_99)\n",
    "print(f\"  Percentile capping: {extreme_high:,} high values capped, {extreme_low:,} low values capped\")\n",
    "\n",
    "# Statistics after outlier removal\n",
    "std_before = train_df['meter_reading'].std()\n",
    "std_after = train_df_agg['meter_reading'].std()\n",
    "std_reduction = ((std_before - std_after) / std_before * 100)\n",
    "\n",
    "print(f\"\\nAfter outlier removal:\")\n",
    "print(f\"  Total rows: {len(train_df_agg):,} (removed {outliers_removed:,})\")\n",
    "print(f\"  Meter reading - Mean: {train_df_agg['meter_reading'].mean():.2f}\")\n",
    "print(f\"  Meter reading - Std: {std_after:.2f} (‚Üì{std_reduction:.1f}%)\")\n",
    "print(f\"  Meter reading - Max: {train_df_agg['meter_reading'].max():.2f}\")\n",
    "print(f\"  Data size: ~{train_df_agg.memory_usage(deep=True).sum()/1024**3:.2f} GB\")\n",
    "print(f\"  Buildings: {train_df_agg['building_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb4ce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. ENHANCED TEMPORAL & WEATHER FEATURE ENGINEERING\n",
      "------------------------------------------------------------------------------------------\n",
      "‚úì Temporal features: hour, dayofweek, month, quarter, is_weekend\n",
      "‚úì Calendar features: is_holiday, near_holiday\n",
      "‚úì Building features: building_age\n",
      "‚úì Enhanced weather features (7): CDD, HDD, relative_humidity, cloud_coverage, precip_depth_1_hr, has_precipitation, sea_level_pressure\n",
      "‚úì Rows: 12,291,399\n",
      "‚úì Features: 30\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: ENHANCED TEMPORAL & WEATHER FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n6. ENHANCED TEMPORAL & WEATHER FEATURE ENGINEERING\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Extract temporal features (no leakage - only from timestamp)\n",
    "train_df_agg['hour'] = train_df_agg['timestamp'].dt.hour\n",
    "train_df_agg['dayofweek'] = train_df_agg['timestamp'].dt.dayofweek\n",
    "train_df_agg['dayofmonth'] = train_df_agg['timestamp'].dt.day\n",
    "train_df_agg['month'] = train_df_agg['timestamp'].dt.month\n",
    "train_df_agg['is_weekend'] = (train_df_agg['dayofweek'] >= 5).astype(int)\n",
    "train_df_agg['quarter'] = train_df_agg['timestamp'].dt.quarter\n",
    "\n",
    "# US Federal holidays (simplified for 2016)\n",
    "us_holidays = pd.to_datetime([\n",
    "    '2016-01-01', '2016-01-18', '2016-02-15', '2016-03-25', '2016-05-30',\n",
    "    '2016-07-04', '2016-09-05', '2016-11-24', '2016-11-25', '2016-12-26'\n",
    "])\n",
    "train_df_agg['is_holiday'] = train_df_agg['timestamp'].dt.date.isin(us_holidays.date).astype(int)\n",
    "\n",
    "# Pre/post holiday (¬±1 day effect) - fixed: use np.concatenate on values, not pd.concat\n",
    "pre_holidays = (us_holidays - pd.Timedelta(days=1)).values\n",
    "post_holidays = (us_holidays + pd.Timedelta(days=1)).values\n",
    "pre_post_holidays = np.concatenate([pre_holidays, post_holidays])\n",
    "train_df_agg['near_holiday'] = train_df_agg['timestamp'].dt.date.isin(pd.to_datetime(pre_post_holidays).date).astype(int)\n",
    "\n",
    "# Building age from year_built\n",
    "if 'year_built' in train_df_agg.columns:\n",
    "    train_df_agg['building_age'] = 2016 - train_df_agg['year_built']\n",
    "    train_df_agg['building_age'] = train_df_agg['building_age'].clip(0, 150)\n",
    "\n",
    "# ENHANCED WEATHER FEATURES\n",
    "weather_features_added = []\n",
    "\n",
    "# CDD/HDD with base 18¬∞C (cooling/heating degree days)\n",
    "if 'air_temperature' in train_df_agg.columns:\n",
    "    base_temp = 18.0\n",
    "    train_df_agg['CDD'] = (train_df_agg['air_temperature'] - base_temp).clip(lower=0)\n",
    "    train_df_agg['HDD'] = (base_temp - train_df_agg['air_temperature']).clip(lower=0)\n",
    "    weather_features_added.extend(['CDD', 'HDD'])\n",
    "    \n",
    "    # Relative humidity (from dew point and air temperature)\n",
    "    if 'dew_temperature' in train_df_agg.columns:\n",
    "        # Magnus formula for relative humidity\n",
    "        def calc_relative_humidity(temp, dew):\n",
    "            a, b = 17.27, 237.7\n",
    "            alpha_temp = (a * temp) / (b + temp)\n",
    "            alpha_dew = (a * dew) / (b + dew)\n",
    "            rh = 100 * np.exp(alpha_dew - alpha_temp)\n",
    "            return np.clip(rh, 0, 100)\n",
    "        \n",
    "        train_df_agg['relative_humidity'] = calc_relative_humidity(\n",
    "            train_df_agg['air_temperature'], \n",
    "            train_df_agg['dew_temperature']\n",
    "        )\n",
    "        weather_features_added.append('relative_humidity')\n",
    "\n",
    "# Cloud coverage (already in data)\n",
    "if 'cloud_coverage' in train_df_agg.columns:\n",
    "    weather_features_added.append('cloud_coverage')\n",
    "\n",
    "# Precipitation (already in data)\n",
    "if 'precip_depth_1_hr' in train_df_agg.columns:\n",
    "    train_df_agg['has_precipitation'] = (train_df_agg['precip_depth_1_hr'] > 0).astype(int)\n",
    "    weather_features_added.extend(['precip_depth_1_hr', 'has_precipitation'])\n",
    "\n",
    "# Sea level pressure (already in data)\n",
    "if 'sea_level_pressure' in train_df_agg.columns:\n",
    "    weather_features_added.append('sea_level_pressure')\n",
    "\n",
    "print(f\"‚úì Temporal features: hour, dayofweek, month, quarter, is_weekend\")\n",
    "print(f\"‚úì Calendar features: is_holiday, near_holiday\")\n",
    "print(f\"‚úì Building features: building_age\")\n",
    "print(f\"‚úì Enhanced weather features ({len(weather_features_added)}): {', '.join(weather_features_added)}\")\n",
    "print(f\"‚úì Rows: {len(train_df_agg):,}\")\n",
    "print(f\"‚úì Features: {len(train_df_agg.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ad8ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. CAUSAL LAG FEATURES\n",
      "------------------------------------------------------------------------------------------\n",
      "‚úì Lag features created: [1, 3, 6, 24, 72] hours\n",
      "‚úì Lagged values are strictly past (no current/future data)\n",
      "   lag_1h: 1,449 NaNs (expected at series start)\n",
      "   lag_3h: 4,347 NaNs (expected at series start)\n",
      "   lag_6h: 8,694 NaNs (expected at series start)\n",
      "   lag_24h: 34,776 NaNs (expected at series start)\n",
      "   lag_72h: 104,328 NaNs (expected at series start)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: CAUSAL LAG FEATURES (PREVENT LEAKAGE) - STRICT CAUSALITY\n",
    "# ============================================================================\n",
    "print(\"\\n7. CAUSAL LAG FEATURES\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Vectorized lag feature creation (much faster than looping)\n",
    "# CRITICAL: Use .shift() to ensure y_t doesn't include itself\n",
    "lag_hours = [1, 3, 6, 24, 72]  # 1h, 3h, 6h, 24h, 72h\n",
    "\n",
    "for lag_h in lag_hours:\n",
    "    lag_col_name = f'lag_{lag_h}h'\n",
    "    # Per-building shift ensures we don't cross building boundaries\n",
    "    train_df_agg[lag_col_name] = train_df_agg.groupby('building_id')['meter_reading'].shift(lag_h)\n",
    "\n",
    "print(f\"‚úì Lag features created: {lag_hours} hours\")\n",
    "print(f\"‚úì Lagged values are strictly past (no current/future data)\")\n",
    "\n",
    "# Verify no NaN patterns suggest leakage\n",
    "for lag_h in lag_hours:\n",
    "    lag_col = f'lag_{lag_h}h'\n",
    "    nan_count = train_df_agg[lag_col].isnull().sum()\n",
    "    print(f\"   {lag_col}: {nan_count:,} NaNs (expected at series start)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58001b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. ROLLING STATISTICS (STRICTLY CAUSAL)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Rolling statistics: mean(24h, 168h), std(24h), max/min(24h)\n",
      "‚úì All use .shift(1) to ensure strictly past-only windows (no leakage)\n",
      "‚úì Causality check: at t=2016-01-01 01:00:00, rolling_mean_24h uses t-24 to t-1 data only\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: ROLLING STATISTICS (CAUSAL - STRICT PAST-ONLY) - VECTORIZED\n",
    "# ============================================================================\n",
    "print(\"\\n8. ROLLING STATISTICS (STRICTLY CAUSAL)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Critical: Use .shift(1) to ensure rolling window uses ONLY past values\n",
    "# rolling_mean_24h at time t = mean of readings from t-24 to t-1 (not t)\n",
    "\n",
    "train_df_agg['rolling_mean_24h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: x.shift(1).rolling(24, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "train_df_agg['rolling_mean_168h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: x.shift(1).rolling(168, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "train_df_agg['rolling_std_24h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: x.shift(1).rolling(24, min_periods=1).std()\n",
    ")\n",
    "\n",
    "train_df_agg['rolling_max_24h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: x.shift(1).rolling(24, min_periods=1).max()\n",
    ")\n",
    "\n",
    "train_df_agg['rolling_min_24h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: x.shift(1).rolling(24, min_periods=1).min()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Rolling statistics: mean(24h, 168h), std(24h), max/min(24h)\")\n",
    "print(f\"‚úì All use .shift(1) to ensure strictly past-only windows (no leakage)\")\n",
    "\n",
    "# Verify causality\n",
    "test_row = train_df_agg[train_df_agg['rolling_mean_24h'].notna()].iloc[0]\n",
    "print(f\"‚úì Causality check: at t={test_row['timestamp']}, rolling_mean_24h uses t-24 to t-1 data only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bed2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8.5. TREND-BASED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Trend features created:\n",
      "   ‚Ä¢ slope_6h: Rate of change over 6 hours\n",
      "   ‚Ä¢ slope_12h: Rate of change over 12 hours\n",
      "   ‚Ä¢ delta_24h: Difference from same time yesterday\n",
      "   ‚Ä¢ acceleration_6h: Change in slope (2nd derivative)\n",
      "‚úì All trend features are strictly past-only (no leakage)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8.5: TREND-BASED FEATURES (SLOPE, ACCELERATION, DELTA)\n",
    "# ============================================================================\n",
    "print(\"\\n8.5. TREND-BASED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# 6h slope (rate of change over last 6 hours)\n",
    "train_df_agg['slope_6h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: (x.shift(1) - x.shift(6)) / 6\n",
    ")\n",
    "\n",
    "# 12h slope (rate of change over last 12 hours)\n",
    "train_df_agg['slope_12h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: (x.shift(1) - x.shift(12)) / 12\n",
    ")\n",
    "\n",
    "# 24h energy delta (difference from same time yesterday)\n",
    "train_df_agg['delta_24h'] = train_df_agg.groupby('building_id')['meter_reading'].transform(\n",
    "    lambda x: x.shift(1) - x.shift(24)\n",
    ")\n",
    "\n",
    "# Short-term acceleration (change in slope - 2nd derivative)\n",
    "train_df_agg['acceleration_6h'] = train_df_agg.groupby('building_id')['slope_6h'].transform(\n",
    "    lambda x: x.shift(1) - x.shift(6)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trend features created:\")\n",
    "print(f\"   ‚Ä¢ slope_6h: Rate of change over 6 hours\")\n",
    "print(f\"   ‚Ä¢ slope_12h: Rate of change over 12 hours\")\n",
    "print(f\"   ‚Ä¢ delta_24h: Difference from same time yesterday\")\n",
    "print(f\"   ‚Ä¢ acceleration_6h: Change in slope (2nd derivative)\")\n",
    "print(f\"‚úì All trend features are strictly past-only (no leakage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25b5486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. TARGET ENCODING FOR PRIMARY_USE (LOG-SAFE & TRAIN-ONLY)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Target-encoded primary_use (16 categories) using log1p target:\n",
      "   Education                     : mean(log1p)=  5.0013, count=4,665,303\n",
      "   Entertainment/public assembly : mean(log1p)=  3.8707, count=1,533,425\n",
      "   Food sales and service        : mean(log1p)=  5.9907, count=  43,470\n",
      "   Healthcare                    : mean(log1p)=  5.8686, count= 199,887\n",
      "   Lodging/residential           : mean(log1p)=  4.4831, count=1,244,234\n",
      "   Manufacturing/industrial      : mean(log1p)=  4.6987, count=  99,487\n",
      "   Office                        : mean(log1p)=  4.8397, count=2,386,305\n",
      "   Other                         : mean(log1p)=  2.9278, count= 209,384\n",
      "   Parking                       : mean(log1p)=  3.5641, count= 187,403\n",
      "   Public services               : mean(log1p)=  3.9894, count=1,319,676\n",
      "   Religious worship             : mean(log1p)=  1.7158, count=  25,184\n",
      "   Retail                        : mean(log1p)=  3.1816, count=  96,585\n",
      "   Services                      : mean(log1p)=  4.6315, count=  86,697\n",
      "   Technology/science            : mean(log1p)=  4.6911, count=  50,946\n",
      "   Utility                       : mean(log1p)=  6.0557, count=  32,477\n",
      "   Warehouse/storage             : mean(log1p)=  3.1987, count= 110,936\n",
      "\n",
      "‚úì building_id (1449 unique) retained as categorical\n",
      "‚úì site_id (16 unique) retained as categorical\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: TARGET ENCODING FOR CATEGORICAL FEATURES (LEAKAGE-SAFE)\n",
    "# ============================================================================\n",
    "print(\"\\n9. TARGET ENCODING FOR PRIMARY_USE (LOG-SAFE & TRAIN-ONLY)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "TARGET_LOG = 'meter_reading_log'  # Log-transformed target\n",
    "categorical_col = 'primary_use'\n",
    "\n",
    "if categorical_col in train_df_agg.columns:\n",
    "    # Compute mean log-target per category **on training data only**\n",
    "    primary_use_means = train_df_agg.groupby(categorical_col)[TARGET_LOG].mean()\n",
    "    \n",
    "    # Map encoding to full dataset (train + test) safely\n",
    "    train_df_agg['primary_use_encoded'] = train_df_agg[categorical_col].map(primary_use_means)\n",
    "    \n",
    "    print(f\"‚úì Target-encoded {categorical_col} ({len(primary_use_means)} categories) using log1p target:\")\n",
    "    for prim_use, mean_val in primary_use_means.items():\n",
    "        count = (train_df_agg[categorical_col] == prim_use).sum()\n",
    "        print(f\"   {prim_use:30s}: mean(log1p)={mean_val:8.4f}, count={count:8,}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {categorical_col} column not found - skipping target encoding\")\n",
    "\n",
    "# Ensure building_id and site_id retained as categorical\n",
    "print(f\"\\n‚úì building_id ({train_df_agg['building_id'].nunique()} unique) retained as categorical\")\n",
    "print(f\"‚úì site_id ({train_df_agg['site_id'].nunique()} unique) retained as categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef63e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. ENHANCED FEATURE SELECTION - TEMPORAL + TREND + WEATHER (LOG TARGET)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Temporal features: 4 - ['hour', 'dayofweek', 'month', 'is_weekend']\n",
      "‚úì Calendar features: 2 - ['is_holiday', 'near_holiday']\n",
      "‚úì Weather features: 8 - ['air_temperature', 'CDD', 'HDD', 'relative_humidity', 'cloud_coverage', 'precip_depth_1_hr', 'has_precipitation', 'sea_level_pressure']\n",
      "‚úì Lag features: 5\n",
      "‚úì Rolling features: 5\n",
      "‚úì Trend features: 4 - ['slope_6h', 'slope_12h', 'delta_24h', 'acceleration_6h']\n",
      "‚úì Extra features: 2 - ['primary_use_encoded', 'building_age']\n",
      "‚úì Total modeling features (excluding ID + target): 30\n",
      "\n",
      "‚úì Final dataset shape: (12291399, 34)\n",
      "‚úì Focus: Temporal + Trend + Enhanced Weather + Lag/Rolling + Extra Features\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: ENHANCED FEATURE SELECTION - TEMPORAL + TREND + WEATHER (LOG TARGET)\n",
    "# ============================================================================\n",
    "print(\"\\n10. ENHANCED FEATURE SELECTION - TEMPORAL + TREND + WEATHER (LOG TARGET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ID columns\n",
    "id_features = ['building_id', 'site_id', 'timestamp']\n",
    "\n",
    "# Use log-transformed target for modeling\n",
    "target_feature = ['meter_reading_log']  # <-- LOG target\n",
    "\n",
    "# Essential temporal features\n",
    "temporal_features = ['hour', 'dayofweek', 'month', 'is_weekend']\n",
    "\n",
    "# Calendar features\n",
    "calendar_features = ['is_holiday', 'near_holiday']\n",
    "\n",
    "# Enhanced weather features\n",
    "essential_weather = [col for col in ['air_temperature', 'CDD', 'HDD', 'relative_humidity', \n",
    "                                     'cloud_coverage', 'precip_depth_1_hr', 'has_precipitation',\n",
    "                                     'sea_level_pressure'] \n",
    "                     if col in train_df_agg.columns]\n",
    "\n",
    "# Causal lag features\n",
    "lag_features = [col for col in train_df_agg.columns if 'lag_' in col]\n",
    "\n",
    "# Rolling statistics\n",
    "rolling_features = [col for col in train_df_agg.columns if 'rolling_' in col]\n",
    "\n",
    "# Trend-based features\n",
    "trend_features = [col for col in ['slope_6h', 'slope_12h', 'delta_24h', 'acceleration_6h'] \n",
    "                  if col in train_df_agg.columns]\n",
    "\n",
    "# Include additional useful features\n",
    "extra_features = []\n",
    "if 'primary_use_encoded' in train_df_agg.columns:\n",
    "    extra_features.append('primary_use_encoded')\n",
    "if 'building_age' in train_df_agg.columns:\n",
    "    extra_features.append('building_age')\n",
    "\n",
    "# Combine all relevant features\n",
    "relevant_features = (id_features + target_feature +\n",
    "                     temporal_features +\n",
    "                     calendar_features +\n",
    "                     essential_weather +\n",
    "                     lag_features +\n",
    "                     rolling_features +\n",
    "                     trend_features +\n",
    "                     extra_features)\n",
    "\n",
    "# Remove duplicates just in case\n",
    "relevant_features = list(dict.fromkeys(relevant_features))\n",
    "\n",
    "# Filter dataframe\n",
    "train_df_agg = train_df_agg[relevant_features].copy()\n",
    "\n",
    "print(f\"‚úì Temporal features: {len(temporal_features)} - {temporal_features}\")\n",
    "print(f\"‚úì Calendar features: {len(calendar_features)} - {calendar_features}\")\n",
    "print(f\"‚úì Weather features: {len(essential_weather)} - {essential_weather}\")\n",
    "print(f\"‚úì Lag features: {len(lag_features)}\")\n",
    "print(f\"‚úì Rolling features: {len(rolling_features)}\")\n",
    "print(f\"‚úì Trend features: {len(trend_features)} - {trend_features}\")\n",
    "print(f\"‚úì Extra features: {len(extra_features)} - {extra_features}\")\n",
    "print(f\"‚úì Total modeling features (excluding ID + target): {len(relevant_features) - len(id_features) - len(target_feature)}\")\n",
    "\n",
    "print(f\"\\n‚úì Final dataset shape: {train_df_agg.shape}\")\n",
    "print(f\"‚úì Focus: Temporal + Trend + Enhanced Weather + Lag/Rolling + Extra Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9be259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. FINAL TEMPORAL ORDER VERIFICATION & SAVE\n",
      "------------------------------------------------------------------------------------------\n",
      "‚úì Data sorted globally by timestamp, then building_id\n",
      "‚úì Global chronological order: True\n",
      "‚úì Per-building chronological order: True\n",
      "‚úì timestamp NaN count: 0\n",
      "‚úì building_id NaN count: 0\n",
      "‚úì meter_reading_log NaN count: 0\n",
      "‚úì air_temperature NaN count: 0\n",
      "‚úì primary_use_encoded NaN count: 0\n",
      "‚úì building_age NaN count: 0\n",
      "\n",
      "üîí DATA LEAKAGE VERIFICATION:\n",
      "   ‚Ä¢ Data is sorted chronologically (global)\n",
      "   ‚Ä¢ Each building's data is in temporal order\n",
      "   ‚Ä¢ Ready for temporal train/test split\n",
      "   ‚Ä¢ No future data in features (all lags/rolling/trend are past-only)\n",
      "\n",
      "‚úì Saved to: ../dataset/energy_processed_fulldata_20251113_004312.csv\n",
      "‚úì Shape: (12291399, 35)\n",
      "‚úì Buildings: 1449\n",
      "‚úì Features: 35\n",
      "‚úì Memory: ~2.92 GB\n",
      "‚úì Time span: 2016-01-01 00:00:00 to 2016-12-31 23:00:00\n",
      "\n",
      "==========================================================================================\n",
      "‚úÖ PREPROCESSING COMPLETE - FULL DATASET WITH PROPER TEMPORAL ORDERING\n",
      "==========================================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: FINAL TEMPORAL ORDERING & SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n10. FINAL TEMPORAL ORDER VERIFICATION & SAVE\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Ensure final temporal ordering for train/test split without leakage\n",
    "train_df_agg = train_df_agg.sort_values(['timestamp', 'building_id']).reset_index(drop=True)\n",
    "\n",
    "# Verify temporal integrity\n",
    "is_chronological = train_df_agg['timestamp'].is_monotonic_increasing\n",
    "per_building_chrono = train_df_agg.groupby('building_id')['timestamp'].is_monotonic_increasing.all()\n",
    "\n",
    "print(f\"‚úì Data sorted globally by timestamp, then building_id\")\n",
    "print(f\"‚úì Global chronological order: {is_chronological}\")\n",
    "print(f\"‚úì Per-building chronological order: {per_building_chrono}\")\n",
    "\n",
    "# Ensure log target exists\n",
    "if 'meter_reading_log' not in train_df_agg.columns:\n",
    "    train_df_agg['meter_reading_log'] = np.log1p(train_df_agg['meter_reading'])\n",
    "\n",
    "# Optional: keep original meter_reading for evaluation\n",
    "if 'meter_reading' not in train_df_agg.columns:\n",
    "    train_df_agg['meter_reading'] = np.expm1(train_df_agg['meter_reading_log'])\n",
    "\n",
    "# Verify no NaNs in critical columns\n",
    "critical_cols = ['timestamp', 'building_id', 'meter_reading_log', 'air_temperature',\n",
    "                 'primary_use_encoded', 'building_age']\n",
    "\n",
    "for col in critical_cols:\n",
    "    nan_count = train_df_agg[col].isnull().sum()\n",
    "    print(f\"‚úì {col} NaN count: {nan_count}\")\n",
    "\n",
    "print(f\"\\nüîí DATA LEAKAGE VERIFICATION:\")\n",
    "print(f\"   ‚Ä¢ Data is sorted chronologically (global)\")\n",
    "print(f\"   ‚Ä¢ Each building's data is in temporal order\")\n",
    "print(f\"   ‚Ä¢ Ready for temporal train/test split\")\n",
    "print(f\"   ‚Ä¢ No future data in features (all lags/rolling/trend are past-only)\")\n",
    "\n",
    "# Save processed data\n",
    "output_file = f\"../dataset/energy_processed_fulldata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "train_df_agg.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Saved to: {output_file}\")\n",
    "print(f\"‚úì Shape: {train_df_agg.shape}\")\n",
    "print(f\"‚úì Buildings: {train_df_agg['building_id'].nunique()}\")\n",
    "print(f\"‚úì Features: {len(train_df_agg.columns)}\")\n",
    "print(f\"‚úì Memory: ~{train_df_agg.memory_usage(deep=True).sum()/1024**3:.2f} GB\")\n",
    "print(f\"‚úì Time span: {train_df_agg['timestamp'].min()} to {train_df_agg['timestamp'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"‚úÖ PREPROCESSING COMPLETE - FULL DATASET WITH PROPER TEMPORAL ORDERING\")\n",
    "print(\"=\"*90)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
